# -*- coding: utf-8 -*-
"""R_en_Python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aMJf_4ROEbvkuxOFJ8hyxzHtCR9o6IAV

# RLM  y R en Python
"""

#Librerias

#Si no lo está, usamos !pip install "paquetería"
!pip install faraway

import pandas as pd # manipulación de tablas
import numpy as np  # calculo numeríco y análisis de datos
import matplotlib.pyplot as plt # generar plot
import statsmodels.api as sm
import statsmodels.formula.api as smf
import seaborn as sns  #permite generar graficos estadisticos
from scipy import stats
import faraway.utils
import statsmodels.stats.diagnostic as sms
from statsmodels.stats.stattools import jarque_bera
from google.colab import drive

#Conectamos con Drive
drive.mount('/content/gdrive')

#Obtenemos nuestros datos
df = pd.read_csv("/content/gdrive/MyDrive/datos.csv")

#Cambiamos el nombre de la variable x1
df = df.rename(columns = {"x1": "Edad"})

#Obtenemos variables dummies para nuestra variable categorica Tratamiento
df_rlm = pd.get_dummies(df, columns=["Tratamiento"])
df_rlm = df_rlm.astype(int)

#Ajustamos el modelo
fit2 =smf.ols("y~Edad+C(Tratamiento_B)+C(Tratamiento_C)+Edad:C(Tratamiento_B)+Edad:C(Tratamiento_C)", df_rlm).fit()
print(fit2.summary())

"""# Verificación de supuestos

¿Cómo verificar que nuestro modelo cumpla los supuestos?

##Linealidad
"""

#POdemos realizar la siguiente gráfica para verificar la linealidad
plt.scatter(fit2.fittedvalues, fit2.resid,marker='x')
plt.xlabel("Fitted values")
plt.ylabel("Residuals")
plt.axhline(0)

"""# Homocedasticidad
Para este supuesto, realizaremos un preprocesamiento, dado que debemos utilizar los residuales, y la matriz de diseño
"""

#Vamos a retomar la base de datos df_rlm, y haremos
df_completo = df_rlm.drop(["y", "Tratamiento_A"], axis=1)
df_completo["Tratamiento_B:Edad"] = df_completo["Edad"]*df_completo["Tratamiento_B"]
df_completo["Tratamiento_C:Edad"] = df_completo["Edad"]*df_completo["Tratamiento_C"]
df_completo.head()

#Tenemos las siguiente prueba de hipótesis
#Prueba de heterocedasticidad de Breusch-Pagan

sms.het_breuschpagan(fit2.resid, sm.add_constant(df_completo))[3]

#Tenemos esta otra prueba de hipótesis
#Prueba de heterocedasticidad de White

sms.het_white(fit2.resid, sm.add_constant(df_completo))[3]

"""Tras la realización de las pruebas, llegamos a que no se rechaza la homocedasticidad, por lo que podemos continuar con la verifiación de supuestos

# Normalidad
Como último supuesto a verificar en esta ocasión, vamos a verificar la normalidad, como sabemos, debemos utilizar los residuales estudentizados, por lo que deberemos calcularlos previo a las pruebas de hipótesis, sin embargo, hay maneras fáciles de calcular
"""

#Podemos guardar la información de los residuales, con la función outlier_test
#porque ésta utiliza los resiudales para determinar si un valor es atípico
res_est = fit2.outlier_test()
res_est.head()

#Realizamos pues las pruebas de hipótesis con los residuales
#Podemos usar lilliefors
sms.lilliefors(res_est.student_resid)[1] #Obtenemos sólo el p-value

#Otra forma de realizar la prueba lilliefors
sms.kstest_normal(res_est.student_resid)[1] #Mismo p-value

#Ahora calculamos la prueba jarque_bera
jarque_bera(res_est.student_resid)[1] #Obtenemos el p-value
#Aunque ya tenemos una aproximación en el summary

"""De ninguna de las pruebas obtuvimos que se rechace la normalidad, por lo que podemos asumir la normalidad para trabajar.

Y ya con esto, vamos a realizar la prueba F asociada a la tabla ANOVA
"""

#Para realizar pruebas F se utiliza el método f_test
A = np.identity(len(fit2.params))
#Quitamos el intercepto
print(A,"\n")
A = A[1:,:]
print(A)
#Realizamos la prueba
print(fit2.f_test(A))

#También podemos obtenerlo directamente del summary
#fit2.f_pvalue

fit2.f_pvalue

"""Tenemos un p-value menor que .05, por lo que al menos una de nuestras variables es significativa.

## Extensión a un GLM
Como bien sabemos, nosotros también podemos utilizar los modelos lineales generalizados para trabajar con datos, presentamos a continuación un ejemplo, aunque no se aleja en demasía de lo previamente realizado
"""

df.head()

#Realizamos el ajuste
fit4 = smf.glm("y~Edad*Tratamiento", data=df, family = sm.families.Gaussian()).fit()
#Cambio de liga
#link=sm.families.links.log()
print(fit4.summary())

#Obtuvimos lo mismo que de la RLM
fit2.sumary()

#La diferencia radica, en que debemos recordar que usamos la aproximación
#chi cuadrada, para las pruebas de hipótesis, por lo que, usamos la prueba wald
print(A)
#Utilizamos la misma escritura que con la F_test
print(fit4.wald_test(A,scalar = True))

#Y en este caso, los supuestos se verificarían igual, pero hay que tener
#cuidado para cada glm

"""# Pruebas de hipótesis SImultáneas

Como ya hemos visto hasta ahora, hemos podido ajustar y verificar nuestro modelo de regresión lineal múltiple, sin embargo, no hemos llegado a una de las herramientas más importantes que tenemos

Las pruebas de hipótesis simultáneas y los intervalos de confianza simultáneos

## ¿Cómo lo hacemos en python?
La respuesta corta es que no se puede, esto porque no hay un módulo propio de una paquetería que nos ayude.

Pero la respuesta larga es que sí podemos hacerlo, sin embargo, para eso necesitamos utilizar R a través de Python, que es lo que veremos a continuación
"""

#La paquetería que utilizare,os es rpy2
import rpy2
print( rpy2 . __version__ )

#Cargamos las cosas que vamos a necesitar
# Las siguientes lineas de permiten utilizar objetos y funciones de R en el script de codigo python
from rpy2.robjects import FloatVector
from rpy2.robjects.packages import importr
from rpy2 import robjects

# Se prepara un ambiente para la instalacion y carga de paquetes
# import rpy2's package module
import rpy2.robjects.packages as rpackages

# import R's utility package
utils = rpackages.importr('utils')

# select a mirror for R packages
utils.chooseCRANmirror(ind=1) # select the first mirror in the list

# R package names
packnames = ('ggplot2', 'multcomp')

# R vector of strings
from rpy2.robjects.vectors import StrVector

# Se verifica si los paquetes han sido previamente instalados, en caso de no existir en nuetsro ambiente
# serán instalados.

names_to_install = [x for x in packnames if not rpackages.isinstalled(x)]
if len(names_to_install) > 0:
    utils.install_packages(StrVector(names_to_install))

#Especificamente, guardamos los objetos siguientes
#Recordamos que debemos usar los módulos
stats = importr('stats')
base = importr('base')
multcomp = importr('multcomp')

df

#Primero debemos guardar nuestros objetos, como objetos de R
y = FloatVector(df.y)
x1 = FloatVector(df.Edad)
xTratamiento = StrVector(df.Tratamiento)
#De momento son objetos sólo en python, hay que guardarlos en R
robjects.globalenv["y"] = y
robjects.globalenv["x1"] = x1
robjects.globalenv["Tratamiento"] = xTratamiento

#Realizamos el ajuste ahora que forman parte del ambiente de R
fit = stats.lm(formula = "y ~ x1*Tratamiento")

print(base.summary(fit))
#Usamos "base" porque es una función de R base

"""Podemos ver que en efecto funciona igual que como lo hacía en R (salvo por los detalles adicionales del print)

$E[y; x1, T ratamiento] = β0 + β1x1 + β2T rat_B + β3T rat_C + β4x1T rat_B + β5x1T rat_C$

$E[y; x1, T ratamiento = A] = β0 + β1x1$

$E[y; x1, T ratamiento = B] = β0 + β1x1 + β2 + β4x1 = (β0 + β2) + x1(β1 + β4)$

$E[y; x1, T ratamiento = C] = β0 + β1x1 + β3 + β5x1 = (β0 + β3) + x1(β1 + β5)$

¿Las pendientes son iguales?

¿El efecto de la variable x1 es la misma para todos los tratamientos?
"""

#Primero vamos a intentar reducir el modelo
#Es decir, buscaremos eliminar las variables de interacción de edad con tratamiento B y C
#Para eso vamos a usar primero la prueba F

#Generamos la identidad
A = np.identity(len(fit2.params))
#Nos quedamos con los últimos coeficientes
print(A,"\n")

A = A[4:,:]
print(A)
#La prueba nos dice que al menos una variable es relevante
#Vamos ahora a ver si sólo una variable es relevante
print(fit2.f_test(A))

#Guardamos ahora en R los datos, pues debe ser en multcomp

#Recordando que ahora debemos agregar redundancias, como que sean la misma interacción
K = robjects.FloatVector([0,0,0,
                          0,0,0,
                          0,0,0,
                          0,0,0,
                          1,0,1,
                          0,1,-1]) #Los datos se meten por columna
K = robjects.r['matrix'](K, ncol=6, nrow=3)
print(K)
m = robjects.FloatVector([0,0,0])
m = robjects.r['matrix'](m, ncol=1)
print(m)
# Y ya tenemos los datos como solemos usar en R

#Y ya los hicimos objetos de R
print(base.summary(multcomp.glht(fit, linfct=K, rhs=m)))

"""Podemos ver que parece ser que el tratamiento A y B interactúan de la misma forma con la edad, y podemos tener un modelo
más pequeño.

## Extras:
Además de lo que ya vimos, una paquetería que también puede ser util es SciKit Learn, la cuál es una paquetería enfocada enteramente en el machin learning, sin embargo, hay muchas funciones útiles que se pueden llevar a cabo con dicha paquetería

En particular, toda su tarea 3 la podrían llevar a cabo con dicha paquetería (solamente siguiendo las buenas prácticas que aprenderán en dicho tema)
"""

#Ejemplo con estos mismos datos
from sklearn import linear_model
reg = linear_model.LinearRegression()

#En particular, estos métodos sólo utilizan matrices numéricas
#Por lo que hay que dummificar
#Además hay que añadir las interacciones y la columna de beta0
df_rlm = pd.get_dummies(df, columns=["Tratamiento"])
#display(df_rlm.head())
df_rlm['Edad:TratamientoB'] = df_rlm["Edad"]*df_rlm["Tratamiento_B"]
df_rlm['Edad:TratamientoC'] = df_rlm["Edad"]*df_rlm["Tratamiento_C"]

respuesta = df_rlm['y']
covariables = df_rlm[[i for i in df_rlm.columns if i != "y" and i != "Tratamiento_A"]]
covariables.head()

#Pero en este caso, no se toma en cuenta al intercepto
#Pero se obtiene de otra forma

reg.fit(covariables, respuesta)
print(reg.coef_)
#Edad TratB TratC EdTratB EdTratC
print(f"Intercepto:",reg.intercept_)

#Creamos las gráficas del modelo co scikit-learn
def graficar_A_SKL(Edad):
  return reg.intercept_+reg.coef_[0]*Edad

def graficar_B_SKL(Edad):
  return reg.intercept_+reg.coef_[0]*Edad+reg.coef_[1]+reg.coef_[3]*Edad

def graficar_C_SKL(Edad):
  return reg.intercept_+reg.coef_[0]*Edad+reg.coef_[2]+reg.coef_[4]*Edad

#Ahora graficamos la información
x=range(15,70)

plt.plot(x, [graficar_A_SKL(i) for i in x])
plt.scatter(df[df['Tratamiento']=="A"].Edad, df[df['Tratamiento']=="A"].y)

plt.plot(x, [graficar_B_SKL(i) for i in x])
plt.scatter(df[df['Tratamiento']=="B"].Edad, df[df['Tratamiento']=="B"].y)

plt.plot(x, [graficar_C_SKL(i) for i in x])
plt.scatter(df[df['Tratamiento']=="C"].Edad, df[df['Tratamiento']=="C"].y)